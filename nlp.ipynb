{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10853344,"sourceType":"datasetVersion","datasetId":6741081}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context-sensitive Spelling Correction\n\nThe goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n\nSubmit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n\nUseful links:\n- [Norvig's solution](https://norvig.com/spell-correct.html)\n- [Norvig's dataset](https://norvig.com/big.txt)\n- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n\nGrading:\n- 60 points - Implement spelling correction\n- 20 points - Justify your decisions\n- 20 points - Evaluate on a test set\n","metadata":{"id":"DIgM6C9HYUhm"}},{"cell_type":"markdown","source":"## Implement context-sensitive spelling correction\n\nYour task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n\nThe best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n\nWhen solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n\n- solving a problem of n-grams frequencies storing for a large corpus;\n- taking into account keyboard layout and associated misspellings;\n- efficiency improvement to make the solution faster;\n- ...\n\nPlease don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n\n##### IMPORTANT:  \nYour project should not be a mere code copy-paste from somewhere. You must provide:\n- Your implementation\n- Analysis of why the implemented approach is suggested\n- Improvements of the original approach that you have chosen to implement","metadata":{"id":"x-vb8yFOGRDF"}},{"cell_type":"code","source":"import os\n\ndata_files = [\"/kaggle/input/ass1nlp/bigrams (2).txt\"]\n\n\ndef load_ngrams(file_path):\n    ngram_dict = {}\n    with open(file_path, \"r\", encoding=\"latin-1\") as f:\n        for line in f:\n            parts = line.strip().split()  \n            if len(parts) > 1:\n                freq = int(parts[0])  \n                ngram = tuple(parts[1:])  \n                ngram_dict[ngram] = freq\n    return ngram_dict\n\n\nngram_data = {}\nfor file in data_files:\n    file_path = file\n    ngram_data[file] = load_ngrams(file_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:10:06.611504Z","iopub.execute_input":"2025-02-26T00:10:06.611849Z","iopub.status.idle":"2025-02-26T00:10:08.194429Z","shell.execute_reply.started":"2025-02-26T00:10:06.611825Z","shell.execute_reply":"2025-02-26T00:10:08.192985Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"from collections import defaultdict\nbigrams = ngram_data[\"/kaggle/input/ass1nlp/bigrams (2).txt\"]\ndef extract_unigrams_from_ngrams(bigrams, fivegrams):\n    unigrams = defaultdict(int)\n\n    for (w1, w2), freq in bigrams.items():\n        unigrams[w1] += freq\n        unigrams[w2] += freq\n    return unigrams\n\n\nunigrams = extract_unigrams_from_ngrams(bigrams, fivegrams)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:10:10.341699Z","iopub.execute_input":"2025-02-26T00:10:10.342032Z","iopub.status.idle":"2025-02-26T00:10:10.866552Z","shell.execute_reply.started":"2025-02-26T00:10:10.342009Z","shell.execute_reply":"2025-02-26T00:10:10.865533Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"def key_distance(c1: str, c2: str) -> float:\n    coord = {\n        'q': (0,0), 'w': (0,1), 'e': (0,2), 'r': (0,3), 't': (0,4), 'y': (0,5), 'u': (0,6), 'i': (0,7), 'o': (0,8), 'p': (0,9),\n        'a': (1,0), 's': (1,1), 'd': (1,2), 'f': (1,3), 'g': (1,4), 'h': (1,5), 'j': (1,6), 'k': (1,7), 'l': (1,8),\n        'z': (2,0), 'x': (2,1), 'c': (2,2), 'v': (2,3), 'b': (2,4), 'n': (2,5), 'm': (2,6)\n    }\n    if c1 not in coord or c2 not in coord:\n        return float('inf') \n    \n    x1, y1 = coord[c1]\n    x2, y2 = coord[c2]\n    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:10:25.787242Z","iopub.execute_input":"2025-02-26T00:10:25.787638Z","iopub.status.idle":"2025-02-26T00:10:25.797227Z","shell.execute_reply.started":"2025-02-26T00:10:25.787610Z","shell.execute_reply":"2025-02-26T00:10:25.795828Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"def get_keyboard_weight(c1: str, c2: str) -> float:\n    dist = key_distance(c1, c2)\n    return 1.0 / (1 + dist**2) if dist != 0 else 0.0\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:10:31.348175Z","iopub.execute_input":"2025-02-26T00:10:31.348683Z","iopub.status.idle":"2025-02-26T00:10:31.354021Z","shell.execute_reply.started":"2025-02-26T00:10:31.348641Z","shell.execute_reply":"2025-02-26T00:10:31.352587Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"print(keyboard_proximity[('d','b')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:10:49.281073Z","iopub.execute_input":"2025-02-26T00:10:49.281690Z","iopub.status.idle":"2025-02-26T00:10:49.290053Z","shell.execute_reply.started":"2025-02-26T00:10:49.281635Z","shell.execute_reply":"2025-02-26T00:10:49.288816Z"}},"outputs":[{"name":"stdout","text":"0.17\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"import itertools\n\n\nkeyboard_chars = \"qwertyuiopasdfghjklzxcvbnm\"\n\nkeyboard_proximity = {}\nfor c1 in keyboard_chars:\n    for c2 in keyboard_chars:\n        if c1 != c2:\n            weight = get_keyboard_weight(c1, c2)\n            if weight > 0.1:  \n                keyboard_proximity[(c1, c2)] = round(weight, 2)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:11:09.068105Z","iopub.execute_input":"2025-02-26T00:11:09.068496Z","iopub.status.idle":"2025-02-26T00:11:09.076404Z","shell.execute_reply.started":"2025-02-26T00:11:09.068466Z","shell.execute_reply":"2025-02-26T00:11:09.075394Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import re\nfrom collections import defaultdict, Counter\nimport itertools\n\n\n\ndef edits1(word: str) -> set:\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word: str) -> set:\n    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n\n\nfrom collections import defaultdict\nimport re\nimport math\nclass SpellCorrector:\n    def __init__(self, unigrams, bigrams, beam_width, keyboard_proximity):\n        self.unigrams = unigrams  \n        self.bigrams = bigrams   \n        self.beam_width = beam_width\n        self.total_uni = sum(unigrams.values())\n        \n        self.keyboard_proximity = keyboard_proximity\n\n    def P(self, word: str) -> float:\n        return self.unigrams.get(word, 0) / self.total_uni\n\n    def count_probs(self, bigram_key, word, context):\n        bigram_freq = self.bigrams.get(bigram_key, 0)\n        p_word = self.unigrams.get(word, 0)\n        p_context = self.unigrams.get(context, 0) \n        if p_context > 0:\n            return (bigram_freq/p_word) * (p_word/self.total_uni)/ (p_context/self.total_uni)       \n        return self.P(word)\n\n    \n    def get_ngram_prob(self, context: tuple, word: str, index) -> float:\n        prob = 0\n        if index == 0:\n            context = list(context)[0]\n            bigram_key = (word,context)\n            prob = self.count_probs(bigram_key, word, context)\n        elif index > 0 and index < len(context):\n            context1 = list(context)[index-1]\n            bigram_key = (context1,word)\n            prob = self.count_probs(bigram_key, word, context)\n            context = list(context)[index]\n            bigram_key = (word,context)\n            prob += self.count_probs(bigram_key, word, context)\n            prob /=2\n        elif index == len(context):\n            context = list(context)[index-1]\n            bigram_key = (context,word)\n            prob = self.count_probs(bigram_key, word, context)\n                 \n        return prob\n\n\n    def score(self, candidate: str, context: list, word, index) -> float:\n\n        key_weight = 1\n        if len(candidate) == len(word):\n            for c_cand, c_prev in zip(candidate, word):\n                if c_cand == c_prev:\n                    key_weight += 1\n                else:\n                    key_weight += self.keyboard_proximity.get((c_prev, c_cand), 0)\n            key_weight -= 1\n        key_weight /= len(word)  \n        \n\n        context_tuple = tuple(context)\n        ngram_prob = self.get_ngram_prob(context_tuple, candidate, index)\n\n        return (0.9 * ngram_prob + 0.1 * self.P(candidate)) * key_weight\n\n    def correct(self, sentence: str) -> str:\n        words = sentence.lower().split()\n        beam = [([], 1.0)]  \n        \n        for idx in range(len(words)):\n            current_word = words[idx]\n            new_beam = []\n            \n            for corrected_seq, score_so_far in beam:\n                candidates = (edits1(current_word) | {current_word} | edits2(current_word)) & self.unigrams.keys()\n                \n                if not candidates:\n                    new_corrected = corrected_seq + [current_word]\n                    new_beam.append((new_corrected, score_so_far))\n                    continue\n                \n                for candidate in candidates:\n                    context_for_score = corrected_seq + words[idx+1:]\n                    candidate_score = self.score(candidate, context_for_score, current_word, idx)\n                    \n                    if candidate == current_word:\n                        candidate_score *= 3  \n                    \n                    new_score = score_so_far * candidate_score\n                    new_corrected = corrected_seq + [candidate]\n                    new_beam.append((new_corrected, new_score))\n            \n    \n            new_beam.sort(key=lambda x: -x[1])\n            beam = new_beam[:self.beam_width]\n\n        \n        if not beam:\n            return sentence\n        \n        best_hypothesis = max(beam, key=lambda x: x[1])\n        return \" \".join(best_hypothesis[0])\n\n\n\n","metadata":{"id":"MoQeEsZvHvvi","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:12:18.559093Z","iopub.execute_input":"2025-02-26T00:12:18.559474Z","iopub.status.idle":"2025-02-26T00:12:18.577952Z","shell.execute_reply.started":"2025-02-26T00:12:18.559448Z","shell.execute_reply":"2025-02-26T00:12:18.576853Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"beam_width = 5\n\ncorrector = SpellCorrector(unigrams, bigrams, beam_width, keyboard_proximity)\n\n\nprint(corrector.correct(\"because you do n't\twamt\")) \nprint(corrector.correct(\"dking woman\")) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:13:11.422208Z","iopub.execute_input":"2025-02-26T00:13:11.422580Z","iopub.status.idle":"2025-02-26T00:13:12.380584Z","shell.execute_reply.started":"2025-02-26T00:13:11.422555Z","shell.execute_reply":"2025-02-26T00:13:12.379335Z"}},"outputs":[{"name":"stdout","text":"because you do n't want\ndying woman\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"## Justify your decisions\n\nWrite down justificaitons for your implementation choices. For example, these choices could be:\n- Which ngram dataset to use\n- Which weights to assign for edit1, edit2 or absent words probabilities\n- Beam search parameters\n- etc.","metadata":{"id":"oML-5sJwGRLE"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# N-gram Dataset Selection\n\nI used a bigram dataset. This choice was made because bigrams allow us to model local word dependencies, improving the accuracy of word correction based on surrounding context. While higher-order n-grams (such as trigrams or five-grams) could provide additional context, they also introduce sparsity issues and require significantly larger datasets.\n\n# Weight Assignments for Edit Probabilities\n\nEdit distances are considered using the edits1 and edits2 functions, which generate candidate words with one or two character modifications. \n\n# Keyboard Proximity Weighting\n\nI implemented a key_distance function to calculate the physical proximity between characters on a QWERTY keyboard. This was integrated into my scoring function to favor corrections that involve adjacent key presses, reflecting common typing mistakes. The weight for keyboard distance is Euclidian distance\n\n# Beam Search Parameters\n\nI used a beam search strategy to efficiently explore possible corrections while maintaining computational feasibility. The beam width was selected to balance performance and accuracy. A wider beam would consider more candidates, increasing correction accuracy, but at the cost of higher computational complexity. Conversely, a narrow beam would reduce processing time but might miss better corrections.\nBecause of this I chose beam width = 5.\n\n# Scoring Function for Candidates\n\nScoring function combines:\n\n-N-gram probability: The likelihood of a candidate word given its surrounding context.\n\n-Unigram probability: The overall frequency of the word in the corpus.\n\n-Keyboard proximity weight: The likelihood that a given word resulted from a nearby key press.\n\nThese factors were weighted to prioritize contextually appropriate words while accounting for common spelling mistakes.\n\n# Test data set\n\nThe test dataset was created using a file containing five grams that were randomly modified to introduce errors. Despite achieving 36% accuracy, the model demonstrates acceptable logical and correct word substitutions.\n","metadata":{"id":"6Xb_twOmVsC6"}},{"cell_type":"markdown","source":"## Evaluate on a test set\n\nYour task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies.","metadata":{"id":"46rk65S4GRSe"}},{"cell_type":"code","source":"import random\ndef introduce_errors(sentence: str, error_prob=0.3) -> str:\n    words = sentence.split()\n    noisy_words = []\n    for word in words:\n        if random.random() < error_prob:\n            noisy_words.append(random.choice(list(edits1(word)) + [word]))\n        else:\n            noisy_words.append(word)\n    return \" \".join(noisy_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:41:57.788804Z","iopub.execute_input":"2025-02-26T00:41:57.789226Z","iopub.status.idle":"2025-02-26T00:41:57.796004Z","shell.execute_reply.started":"2025-02-26T00:41:57.789198Z","shell.execute_reply":"2025-02-26T00:41:57.794409Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"test = {}\nwith open(\"/kaggle/input/ass1nlp/fivegrams (2).txt\", \"r\", encoding=\"latin-1\") as f:\n    for i, line in enumerate(f):\n        if i % 100000 == 0: \n            parts = line.strip().split()\n            if len(parts) > 1:\n                freq = int(parts[0])\n                ngram = \" \".join(parts[1:])  \n                test[ngram] = introduce_errors(ngram)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:41:59.594516Z","iopub.execute_input":"2025-02-26T00:41:59.594898Z","iopub.status.idle":"2025-02-26T00:41:59.921792Z","shell.execute_reply.started":"2025-02-26T00:41:59.594871Z","shell.execute_reply":"2025-02-26T00:41:59.920463Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"\ndef evaluate(corrector, test_sentences):\n    total, correct = 0, 0\n    for k,v in test_sentences.items():\n        corrected_sentence = corrector.correct(v)\n        print(f\"corrected: {corrected_sentence}  true: {k}  error:{v}\")\n        if corrected_sentence == k:\n            correct += 1\n        total += 1\n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:42:04.171752Z","iopub.execute_input":"2025-02-26T00:42:04.172146Z","iopub.status.idle":"2025-02-26T00:42:04.177820Z","shell.execute_reply.started":"2025-02-26T00:42:04.172117Z","shell.execute_reply":"2025-02-26T00:42:04.176694Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"accuracy = evaluate(corrector, test)\nprint(f\"Accuracy: {accuracy:.2%}\")\n","metadata":{"id":"OwZWaX9VVs7B","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T00:42:06.347960Z","iopub.execute_input":"2025-02-26T00:42:06.348430Z","iopub.status.idle":"2025-02-26T00:42:17.031040Z","shell.execute_reply.started":"2025-02-26T00:42:06.348398Z","shell.execute_reply":"2025-02-26T00:42:17.029270Z"}},"outputs":[{"name":"stdout","text":"corrected: a have in the words  true: a babe in the woods  error:a bace in the fwoods\ncorrected: are likely to be quite  true: are likely to be quite  error:are likely to be qyuite\ncorrected: crisis that and to the  true: crisis that led to the  error:criois that led zo the\ncorrected: has been used in a  true: has been used in a  error:has been used int a\ncorrected: in of very good position  true: in a very good position  error:in m vnry good position\ncorrected: looked at the and smiled  true: looked at her and smiled  error:looked at her and smiled\ncorrected: of of the four or  true: one of the four or  error:onz ofl the four or\ncorrected: talk about the decline of  true: talk about the decline of  error:talk aboutm the decline ofi\ncorrected: the strength of the u.s  true: the strength of the u.s  error:kthe strength of thee u.s\ncorrected: to the findings of it  true: to the findings of a  error:to the findings of g\ncorrected: wife and to children and  true: wife and two children in  error:wife and twqo children inr\nAccuracy: 36.36%\n","output_type":"stream"}],"execution_count":127},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Useful resources (also included in the archive in moodle):\n\n1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)","metadata":{}}]}
